"""
Core.py - M√≥dulo Principal del Asistente UDCito
==============================================

Este m√≥dulo implementa la funcionalidad central del asistente virtual de la Universidad del Chubut.
Maneja la carga de configuraci√≥n, la conexi√≥n con OpenAI y la l√≥gica de consultas.

Mejoras implementadas:
‚úÖ Uso del historial en la consulta de recuperaci√≥n.
‚úÖ Reformulaci√≥n de preguntas para mejorar el contexto.
‚úÖ Reordenamiento de documentos recuperados (Reranking).
‚úÖ B√∫squeda h√≠brida (Embeddings + Texto).
‚úÖ Preparaci√≥n para MultiQuery Retriever (sin implementarlo todav√≠a).

Cada una de estas mejoras puede ser comentada si no deseas usarla, sin afectar el resto del c√≥digo.

Autor: Jose Lacunza Kobs
Fecha: Enero 2025
"""

import os
import logging
from pathlib import Path
from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.schema import HumanMessage, AIMessage, SystemMessage
from langchain.retrievers.multi_query import MultiQueryRetriever

# =====================================
# Configuraci√≥n inicial de logging
# =====================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Limpiar cualquier API key existente para evitar conflictos
if 'OPENAI_API_KEY' in os.environ:
    del os.environ['OPENAI_API_KEY']

def load_environment():
    """
    Carga y valida las variables de entorno desde el archivo .env
    
    El archivo .env debe contener:
    - OPENAI_API_KEY: Clave de API de OpenAI
    - EMBEDDING_MODEL_NAME: Nombre del modelo de embeddings
    - MODEL_NAME: Modelo de GPT a utilizar
    - TEMPERATURE: Temperatura para generaci√≥n de respuestas
    - MAX_TOKENS: L√≠mite de tokens en respuestas
    - RETRIEVER_K: N√∫mero de documentos a recuperar
    - DATA_PATH: Ruta a los datos
    - CHROMA_PATH: Ruta a la base de datos Chroma
    
    Returns:
        dict: Diccionario con todas las variables de configuraci√≥n
    """
    # Buscar el archivo .env en la carpeta actual
    env_path = Path('.env')
    
    # Verificar que existe el archivo
    if not env_path.exists():
        raise ValueError("‚ùå Archivo .env no encontrado en la ruta actual")
    
    # Cargar variables de entorno
    load_dotenv(env_path)
    logger.info(f"üìÇ Archivo .env cargado desde: {env_path.absolute()}")
    
    # Validar API key
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        raise ValueError("‚ùå OPENAI_API_KEY no configurada en .env")
    if not api_key.startswith('sk-'):
        raise ValueError("‚ùå OPENAI_API_KEY con formato incorrecto")
    
    logger.info("‚úÖ Variables de entorno cargadas correctamente")
    
    # Retornar configuraci√≥n completa
    return {
        'OPENAI_API_KEY': api_key,
        'EMBEDDING_MODEL_NAME': os.getenv('EMBEDDING_MODEL_NAME', 'text-embedding-3-small'),
        'DATA_PATH': os.getenv('DATA_PATH'),
        'CHROMA_PATH': os.getenv('CHROMA_PATH'),
        'MODEL_NAME': os.getenv('MODEL_NAME', 'gpt-4-turbo-preview'),  # Corregido el modelo por defecto
        'TEMPERATURE': float(os.getenv('TEMPERATURE', '0.7')),
        'MAX_TOKENS': int(os.getenv('MAX_TOKENS', '5000')),
        'RETRIEVER_K': int(os.getenv('RETRIEVER_K', '10'))
    }

def initialize_services():
    """
    Inicializa todos los servicios necesarios para el funcionamiento del asistente:
    1. Embeddings de OpenAI
    2. Base de datos Chroma
    3. Recuperador de documentos
    4. Modelo de lenguaje (LLM)
    
    Returns:
        tuple: (embeddings, db, retriever, llm)
    """
    try:
        # Cargar configuraci√≥n
        config = load_environment()
        
        # Establecer API key en variables de entorno
        os.environ['OPENAI_API_KEY'] = config['OPENAI_API_KEY']
        
        # 1. Inicializar servicio de embeddings
        embeddings = OpenAIEmbeddings(
            openai_api_key=config['OPENAI_API_KEY']
        )
        
        # 2. Validar y configurar base de datos Chroma
        if not config['CHROMA_PATH']:
            raise ValueError("‚ùå CHROMA_PATH no configurado en .env")
        
        db = Chroma(
            persist_directory=config['CHROMA_PATH'],
            embedding_function=embeddings
        )
        
        # 3. Configurar recuperador de documentos
        retriever = db.as_retriever(
            search_type="similarity",
            search_kwargs={"k": config['RETRIEVER_K']}
        )
        
        # 4. Inicializar modelo de lenguaje
        llm = ChatOpenAI(
            model_name=config['MODEL_NAME'],
            temperature=config['TEMPERATURE'],
            max_tokens=config['MAX_TOKENS'],
            openai_api_key=config['OPENAI_API_KEY']
        )
        
        logger.info("‚úÖ Servicios inicializados correctamente")
        return embeddings, db, retriever, llm
        
    except Exception as e:
        logger.error(f"‚ùå Error en inicializaci√≥n: {str(e)}")
        raise

# =====================================
# Inicializaci√≥n Global de Servicios
# =====================================
try:
    embeddings, db, retriever, llm = initialize_services()
except Exception as e:
    logger.error(f"‚ùå Error cr√≠tico durante la inicializaci√≥n: {str(e)}")
    raise

def test_openai_connection():
    """
    Verifica la conexi√≥n con OpenAI realizando una prueba de embeddings
    
    Returns:
        bool: True si la conexi√≥n es exitosa, False en caso contrario
    """
    try:
        _ = embeddings.embed_query("test")
        logger.info("‚úÖ Conexi√≥n con OpenAI verificada")
        return True
    except Exception as e:
        logger.error(f"‚ùå Error de conexi√≥n con OpenAI: {str(e)}")
        return False

def recuperar_documentos(query: str) -> list:
    """
    Recupera documentos relevantes de la base de datos seg√∫n la consulta
    
    Args:
        query (str): Consulta del usuario
        
    Returns:
        list: Lista de documentos relevantes
    """
    try:
        docs = retriever.invoke(input=query)
        for doc in docs:
            logger.info(f"üìÑ Documento recuperado: {doc.metadata}")
        return docs
    except Exception as e:
        logger.error(f"‚ùå Error en recuperaci√≥n: {str(e)}")
        return []

def consultar_llm(context_docs: list, question: str, history: list) -> str:
    """
    Realiza una consulta al modelo de lenguaje usando el contexto y el historial
    
    Args:
        context_docs (list): Lista de documentos de contexto
        question (str): Pregunta del usuario
        history (list): Historial de conversaci√≥n
        
    Returns:
        str: Respuesta del modelo
    """
    try:
        # Unir todo el contexto en un solo texto
        context = "\n".join([doc.page_content for doc in context_docs])
        
        # Crear mensaje del sistema con instrucciones
        system_message = SystemMessage(
            content=(
                "Eres un Chatbot asistente de la Universidad del Chubut. "
                "Responde usando solo la informaci√≥n del siguiente contexto, teniendo en cuenta tambien el historial de conversacion. "
                "Usa un tono formal y profesional. "
                "No inventes informaci√≥n y responde solo con datos verificados. "
                "Si la informaci√≥n no es suficiente, ind√≠calo. "
                f"Contexto: {context}"
            )
        )

        # Construir lista de mensajes con el historial
        messages = [system_message]
        for msg in history:
            if msg['role'] == 'user':
                messages.append(HumanMessage(content=msg['content']))
            elif msg['role'] == 'assistant':
                messages.append(AIMessage(content=msg['content']))

        # Agregar la pregunta actual
        messages.append(HumanMessage(content=question))
        
        # Obtener respuesta del modelo
        logger.info("üí≠ Enviando consulta al LLM...")
        response = llm.invoke(input=messages)
        logger.info("‚úÖ Respuesta recibida")
        
        return response.content

    except Exception as e:
        logger.error(f"‚ùå Error en consulta: {str(e)}")
        return f"Lo siento, ocurri√≥ un error: {str(e)}"

# =====================================
# Verificaci√≥n inicial de conexi√≥n
# =====================================
if not test_openai_connection():
    raise ConnectionError("‚ùå No se pudo establecer conexi√≥n con OpenAI")




# ==========================================
# FUNCIONES AUXILIARES PARA MEJORAS
# ==========================================

def reformular_pregunta(history: list, question: str) -> str:
    """Reformula la pregunta para hacerla m√°s clara y con contexto."""
    try:
        reformulacion = llm.invoke(input=[
            SystemMessage(content="Reformula la pregunta del humano teniendo en cuenta el contexto y el historial, tu respuesta se utilizara para recuperar informacion de la base vectorial y contestar la pregunta del humano."),
            HumanMessage(content=f"Historial: {history[-3:]}"),
            HumanMessage(content=f"Pregunta: {question}")
        ])
        logger.info("‚úÖ Reformulaci√≥n recibida")
        logger.info(f"üîÑ Reformulaci√≥n: {reformulacion.content}")

        return reformulacion.content
    except Exception as e:
        logger.error(f"‚ùå Error en reformulaci√≥n: {str(e)}")
        return question  # Devuelve la original si falla

def reordenar_documentos(query: str, documentos: list) -> list:
    """Reordena los documentos recuperados seg√∫n su relevancia."""
    try:
        documentos_rankeados = sorted(
            documentos,
            key=lambda doc: llm.invoke(input=[
                SystemMessage(content="Eval√∫a la relevancia del documento."),
                HumanMessage(content=f"Consulta: {query}"),
                HumanMessage(content=f"Documento: {doc.page_content}")
            ]).content,
            reverse=True
        )
        return documentos_rankeados
    except Exception as e:
        logger.error(f"‚ùå Error en reordenamiento: {str(e)}")
        return documentos  # Devuelve los documentos en su orden original

def recuperar_documentos_hibrido(query: str, history: list) -> list:
    """Usa b√∫squeda h√≠brida (Embeddings + Texto completo) para mejorar la recuperaci√≥n."""
    try:
        # ‚úÖ Convertir `history` en una lista de strings
        historial_completo = " ".join([msg.content for msg in history[-3:]]) if history else ""

        # ‚úÖ Enriquecer la consulta con historial (si hay)
        consulta_enriquecida = f"{historial_completo} {query}".strip()

        # Recuperaci√≥n sem√°ntica (Embeddings)
        documentos_semanticos = retriever.invoke(input=consulta_enriquecida)

        # Recuperaci√≥n por palabras clave (Texto completo)
        documentos_texto = db.similarity_search(consulta_enriquecida, k=5)

        # ‚úÖ Soluci√≥n: Usar un diccionario para eliminar duplicados
        documentos_unicos = {doc.page_content: doc for doc in documentos_semanticos + documentos_texto}.values()

        return list(documentos_unicos)
    except Exception as e:
        logger.error(f"‚ùå Error en recuperaci√≥n h√≠brida: {str(e)}")
        return []


# ==========================================
# FUNCI√ìN PRINCIPAL DE RECUPERACI√ìN DE DOCUMENTOS
# ==========================================

def recuperar_documentos(query: str, history: list) -> list:
    """
    Recupera documentos relevantes bas√°ndose en la consulta y el historial.
    
    Incorpora mejoras opcionales que pueden comentarse seg√∫n necesidad.
    """
    try:
        # 1Ô∏è‚É£ Reformulaci√≥n de la pregunta (Opcional)
        query = reformular_pregunta(history, query)  # üí¨ Comentar para desactivar

        # 2Ô∏è‚É£ Recuperaci√≥n de documentos con b√∫squeda h√≠brida (Opcional)

        documentos = recuperar_documentos_multiquery(query, history)  # üîç Comentar para usar embbedings
        #documentos = recuperar_documentos_hibrido(query, history)  # üîç Comentar para usar solo embeddings

        # 3Ô∏è‚É£ Reordenamiento de documentos (Opcional)
       # documentos = reordenar_documentos(query, documentos)  # üìÑ Comentar si no quieres usar reranking

        return documentos
    except Exception as e:
        logger.error(f"‚ùå Error en recuperaci√≥n de documentos: {str(e)}")
        return []

# ==========================================
# PREPARACI√ìN PARA MULTIQUERY RETRIEVER
# ==========================================

def recuperar_documentos_multiquery(query: str, history: list) -> list:
    """
    üìå Recupera documentos usando `MultiQuery Retriever`.

    üõ†Ô∏è ¬øC√≥mo funciona?
    1Ô∏è‚É£ Usa un modelo LLM para generar varias versiones de la consulta original.
    2Ô∏è‚É£ Ejecuta cada consulta reformulada en la base de datos vectorial.
    3Ô∏è‚É£ Fusiona los resultados y elimina duplicados.

    Args:
        query (str): Pregunta original del usuario.
        history (list): Historial de conversaci√≥n (no se usa en esta versi√≥n, pero puede usarse en el futuro).

    Returns:
        list: Lista de documentos √∫nicos encontrados.
    """
    try:
        # ‚úÖ Configuramos el retriever MultiQuery con el modelo de lenguaje
        multiquery_retriever = MultiQueryRetriever.from_llm(
            retriever=retriever,  # üîç Base de datos vectorial
            llm=llm,  # üß† Modelo de lenguaje para generar consultas m√∫ltiples
            include_original=True  # ‚úÖ Incluir la consulta original en la b√∫squeda
        )

        # ‚úÖ Ejecutamos la recuperaci√≥n con m√∫ltiples consultas generadas
        documentos_multiquery = multiquery_retriever.get_relevant_documents(query)

        # ‚úÖ Eliminamos duplicados usando `page_content` como clave
        documentos_unicos = {doc.page_content: doc for doc in documentos_multiquery}.values()

        return list(documentos_unicos)

    except Exception as e:
        logger.error(f"‚ùå Error en recuperaci√≥n MultiQuery: {str(e)}")
        return []
